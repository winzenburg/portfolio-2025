# LinkedIn Content Publishing Schedule - 2026
# Showcasing the Product Creation System (Cultivate)
# 
# Publishing Pattern:
# - Tuesdays: Article announcement (business case, big idea)
# - Thursdays: Follow-up engagement (insights, questions, discussion)
#
# Content Theme: Product Creation, AI-Augmented Workflows, Systematic Validation
# Target Audience: Product leaders, founders, VCs, design/engineering leaders
#
# STATUS: Starting Week 1 on January 14, 2026

schedule:
  # === WEEK 1 ===
  week_1:
    start_date: "2026-01-14"  # Tuesday
    article:
      title: "The Problem: Why Most SaaS Startups Fail Before They Even Start"
      slug: "the-problem-why-most-saas-startups-fail"
      file: "cultivate-source/the-problem-why-most-saas-startups-fail-before-they-even-start.md"
      category: "Product Creation"

    posts:
      tuesday:
        publish_date: "2026-01-14"
        type: "article_announcement"
        content: |
          90% of SaaS startups fail. After a decade of building (and failing), I've identified the pattern:

          It's not the idea. It's the process.

          The Build Trap: Founders code for 6 months before validating anyone actually wants it.
          
          The Durability Problem: Even validated ideas fail if they can't sustain paying customers 12-36 months out.
          
          The Portfolio Problem: Betting everything on one idea is suicide. One product fails = startup fails.
          
          The Documentation Problem: Knowledge gets lost. Workflows aren't repeatable. Teams can't scale.
          
          The AI Problem: Tools like ChatGPT are powerful but disconnected from systematic product creation.

          I spent 3 years building a solution: a systematic SaaS creation workflow with 25 agents, 8 quality gates, and zero guesswork. It's managing 16+ product ideas right now—from concept to production.

          Full breakdown of the problems (and the solution framework) in today's article (link in comments).

          #ProductStrategy #SaaS #StartupFailure #SystematicInnovation

      thursday:
        publish_date: "2026-01-16"
        type: "engagement"
        content: |
          Here's a question that makes founders uncomfortable:

          How do you know your validated idea will still matter in 18 months?

          Most founders validate desirability ("people want this now") but ignore durability ("will they still pay for it later?").

          I've seen products with perfect PMF at launch die 12 months later because:
          - The job-to-be-done was infrequent (yearly, not daily)
          - The buyer had no dedicated budget (discretionary, not structural)
          - The problem was trend-driven (hype cycle, not enduring pain)
          - Users could walk away without consequence (no switching costs)

          Now I score every idea on 5 durability metrics before building. Anything below 18/25? I kill it—no matter how excited I am.

          What durability factors do you evaluate before committing capital? Genuinely curious what's working for others.

          #ProductStrategy #StartupValidation #DurabilityThinking

  # === WEEK 2 ===
  week_2:
    start_date: "2026-01-21"  # Tuesday
    article:
      title: "The Complete Workflow: Discovery to Deployment in 2025"
      slug: "the-complete-workflow-discovery-to-deployment"
      file: "cultivate-source/the-complete-workflow-discovery-to-deployment-in-2025.md"
      category: "Product Creation"

    posts:
      tuesday:
        publish_date: "2026-01-21"
        type: "article_announcement"
        content: |
          After 3 years of iteration and failed startups, we built something remarkable:

          A fully systematic SaaS creation workflow that transforms ideas into validated, defensible products with predictable outcomes.

          This isn't a framework we're "planning to implement." This is production, running right now, managing 16+ products at various stages:

          • 25 specialized AI agents coordinated by one orchestrator
          • 8 mandatory quality gates (no idea passes without proving value)
          • 150+ rules organized into a four-tier taxonomy
          • 12 document types capturing every decision
          • Zero ambiguity about what "done" means at each stage

          The result? We go from "here's an idea" to "here's a deployed, validated SaaS product" in a systematic, repeatable way.

          No more guesswork. No more hoping. Just a process that compounds learning and minimizes wasted capital.

          Full workflow breakdown—the gates, the agents, the orchestration—in today's article (link in comments).

          #ProductCreation #SystematicInnovation #SaaS #ProcessDesign

      thursday:
        publish_date: "2026-01-23"
        type: "engagement"
        content: |
          Unpopular opinion: Most "agile" product teams are actually just chaotic.

          They ship fast, pivot often, but have no systematic way to capture learning or compound insights across products.

          Here's what changed for us:

          Every product idea must pass 8 quality gates. Not suggestions—hard gates. If Gate 3 (durability validation) fails, the idea dies. No exceptions.

          Sounds rigid, right? But it's the opposite. The structure creates speed because teams aren't debating "should we build this?" at every standup. The framework already answered that question.

          Now our capacity goes to execution, not endless validation theater.

          The counterintuitive truth: constraints create velocity. Fewer decisions to make = faster movement.

          How does your team decide what to build vs. what to kill? What's your "kill criteria"?

          #ProductLeadership #AgileGoneWrong #SystematicThinking

  # === WEEK 3 ===
  week_3:
    start_date: "2026-01-28"  # Tuesday
    article:
      title: "The Solution: A Dual-Filter Validation Framework"
      slug: "the-solution-dual-filter-validation"
      file: "cultivate-source/the-solution-a-dual-filter-validation-framework.md"
      category: "Product Creation"

    posts:
      tuesday:
        publish_date: "2026-01-28"
        type: "article_announcement"
        content: |
          Watched a team spend 9 months building a product people wanted. It launched, got traction, and died 14 months later.

          Why? They validated desirability. They ignored durability.

          "People want this now" isn't the same as "people will pay for this in 18 months."

          We built a dual-filter validation framework to solve this:

          Filter 1 (Desirability): Does a high-heat tribe desperately want this? Validate with fake doors, concierge MVPs, preorders. If the community wouldn't riot if it disappeared, kill it.

          Filter 2 (Durability): Will this still matter 12-36 months out? Score it on frequency, budget, defensibility, switching costs. If it's below 18/25, kill it—even if desirability is high.

          Both filters must pass. High desirability + low durability = wasted capital. High durability + low desirability = no one cares.

          Only ideas that pass both filters get funding. This framework has prevented us from building at least 8 "good ideas" that would've failed slowly.

          Full dual-filter breakdown in today's article (link in comments).

          #ProductValidation #StartupStrategy #DurabilityThinking

      thursday:
        publish_date: "2026-01-30"
        type: "engagement"
        content: |
          Real talk: How many "validated" ideas have you killed AFTER validation?

          Most founders treat validation as a green light—once it passes, they're all-in. But validation is a continuous process, not a one-time event.

          We've killed 3 products post-validation because durability signals deteriorated:
          - Frequency of the job dropped (users only needed it quarterly, not weekly)
          - Economic buyers lost budget (discretionary spend got cut in downturn)
          - Switching costs never materialized (users churned with zero friction)

          Each time, we caught the signal early because we kept scoring durability every quarter. Killing a validated product still hurts, but it hurts way less than 18 months of sunken capital.

          The discipline: Re-validate durability quarterly. If the score drops below threshold, kill it fast.

          Do you re-validate products after launch? What triggers a "kill" decision for you?

          #ProductLeadership #StartupStrategy #KillYourDarlings

  # === WEEK 4 ===
  week_4:
    start_date: "2026-02-04"  # Tuesday
    article:
      title: "The Portfolio Approach: Managing Multiple Bets, Killing Fast"
      slug: "the-portfolio-approach-multiple-bets"
      file: "cultivate-source/the-portfolio-approach-managing-multiple-bets-killing-fast-prioritizing-by-expected-value.md"
      category: "Product Creation"

    posts:
      tuesday:
        publish_date: "2026-02-04"
        type: "article_announcement"
        content: |
          Solo founders betting everything on one product idea is startup suicide.

          One product fails = entire startup fails. The risk is existential.

          Here's what we do instead: portfolio-based product creation.

          Run 10-16 product ideas in parallel. Most will fail fast (desirability or durability filters). A few will pass validation. One or two will become compounding assets.

          The math is simple:
          - 10 ideas → 3 pass validation → 1 becomes a durable SaaS business
          - 1 idea → fails → game over

          Diversification isn't just for investors—it's for founders managing capital and time risk.

          Our system:
          • Every idea starts with a portfolio score (0-40 points across 8 criteria)
          • Ideas below 20 points never get validated (dead on arrival)
          • Ideas 20-28 get lightweight validation (fake door tests)
          • Ideas 28+ get full validation + brand system + build

          We're managing 16 ideas right now. 11 are in validation. 3 are in build. 2 are in production. The diversification protects us from any single failure.

          Full portfolio approach in today's article (link in comments).

          #PortfolioThinking #StartupStrategy #ProductCreation #RiskManagement

      thursday:
        publish_date: "2026-02-06"
        type: "engagement"
        content: |
          Counterintuitive truth: The more product ideas you validate simultaneously, the faster you move.

          Sounds backwards, right? More ideas = more chaos?

          But here's what actually happens:

          With one idea, every setback feels catastrophic. You over-index on saving it. Sunk cost fallacy kicks in. You keep building even when signals say stop.

          With 10 ideas, failures are just data points. Idea #3 fails durability validation? Kill it. Move resources to Idea #7. No drama.

          The portfolio approach makes you ruthless. And ruthlessness = velocity.

          We killed 8 ideas in the last 6 months. Each one felt fine because we had 8 more in the pipeline. Compare that to founders who spend 18 months on one idea before admitting it's not working.

          What's your mental model for managing multiple product bets? How do you decide where to allocate resources?

          #ProductLeadership #PortfolioThinking #StartupStrategy

  # === WEEK 5 ===
  week_5:
    start_date: "2026-02-11"  # Tuesday
    article:
      title: "The AI Tool Stack: Coordinating 7 AIs Without Chaos"
      slug: "the-ai-tool-stack-seven-ais"
      file: "cultivate-source/the-ai-tool-stack-coordinating-seven-ais-without-chaos.md"
      category: "AI & Automation"

    posts:
      tuesday:
        publish_date: "2026-02-11"
        type: "article_announcement"
        content: |
          Here's the mistake I see constantly:

          Teams adopt ChatGPT, then Claude, then Midjourney, then whatever's trending on Twitter. Each tool gets used ad-hoc. No clear boundaries.

          The result?
          • Tool overlap: 3 AIs doing the same research, burning budget
          • Inconsistent outputs: ChatGPT writes copy, Claude rewrites it differently, no one knows which to use
          • Context loss: Information trapped in tool-specific chats
          • Decision paralysis: "Should I use ChatGPT or Claude for this?"

          We built the opposite: a coordinated AI tool stack with clear lanes, explicit handoffs, and systematic routing.

          We orchestrate 7 AI tools:
          • Manus (orchestration + doc generation)
          • ChatGPT (research, ideation)
          • Claude (strategy, depth)
          • Cursor (code)
          • ElevenLabs (voice)
          • Midjourney + Glif (visuals)
          • Lindy (task automation)

          Each tool has ONE job. No overlap. Information flows through documented pipelines. And we route tasks based on what each tool does best.

          Full AI orchestration breakdown in today's article (link in comments).

          #AITools #ProductivityStack #AIOrchestration #SystematicWork

      thursday:
        publish_date: "2026-02-13"
        type: "engagement"
        content: |
          Real question: How many of your team's AI tool subscriptions are actually providing value?

          We audited ours last month. We were paying for 12 AI tools. Only 7 were being used systematically. The other 5? Occasional use, no clear value.

          The problem wasn't the tools—it was the lack of orchestration. No clear lanes. No routing rules. People just grabbed whatever tool felt convenient.

          Now we have explicit routing logic:
          - Need strategic depth? → Claude
          - Need creative breadth? → ChatGPT
          - Need code? → Cursor
          - Need task automation? → Lindy

          Each tool has a single job. And we killed the tools that didn't fit clear lanes.

          Result: Cut tool spend by 40%, increased output quality, eliminated "which tool should I use?" decision fatigue.

          How does your team decide which AI tool to use for what? Do you have explicit routing rules or is it ad-hoc?

          #AITools #ProductivitySystems #ToolStackOptimization

  # === WEEK 6 ===
  week_6:
    start_date: "2026-02-18"  # Tuesday
    article:
      title: "The AI Orchestration: Connecting Multiple AI Tools Systematically"
      slug: "the-ai-orchestration-systematic-workflow"
      file: "cultivate-source/the-ai-orchestration-how-to-connect-multiple-ai-tools-in-a-systematic-workflow.md"
      category: "AI & Automation"

    posts:
      tuesday:
        publish_date: "2026-02-18"
        type: "article_announcement"
        content: |
          Using multiple AI tools sounds like a nightmare. How do you prevent chaos when 7 different AIs are working on the same project?

          The answer: orchestration.

          We built a system where one AI (Manus) acts as the orchestrator. It coordinates the other 6 tools, enforces quality gates, and ensures outputs flow systematically through the pipeline.

          Here's how it works:

          1. Manus receives a product creation request
          2. It routes research to ChatGPT (breadth) and Claude (depth)
          3. It validates outputs against 150+ rules
          4. It generates structured docs (briefs, strategies, playbooks)
          5. It hands code tasks to Cursor with full context
          6. It routes visual tasks to Midjourney/Glif
          7. It coordinates task automation via Lindy

          Every handoff is explicit. Every output is validated. No tool operates in isolation.

          The result? We create products 4-6x faster than traditional workflows while maintaining higher quality.

          The breakthrough isn't the tools—it's the orchestration layer that makes them systematic.

          Full orchestration breakdown in today's article (link in comments).

          #AIOrchestration #SystematicWork #ProductCreation #AIWorkflows

      thursday:
        publish_date: "2026-02-20"
        type: "engagement"
        content: |
          Controversial take: AI tools without orchestration are just expensive chaos.

          I've seen teams adopt 10+ AI tools and become less productive. Why?

          Because every team member is figuring out their own workflow. ChatGPT for research. Claude for writing. Cursor for code. But no one's coordinating the outputs.

          The result:
          - Redundant work (3 people researching the same thing)
          - Inconsistent quality (everyone's prompts are different)
          - Context loss (information trapped in individual tool conversations)
          - Integration hell (no systematic handoffs between tools)

          The fix: One orchestrator AI that routes tasks, validates outputs, and enforces quality gates.

          It's the difference between "everyone has power tools" and "everyone's building the same house with a coordinated blueprint."

          How does your team coordinate AI tool usage? Is it orchestrated or ad-hoc?

          #AIOrchestration #ProductivitySystems #TeamWorkflow

  # === WEEK 7 ===
  week_7:
    start_date: "2026-02-25"  # Tuesday
    article:
      title: "The Rules & Agents System: Making the Process Repeatable"
      slug: "the-rules-agents-system-repeatable"
      file: "cultivate-source/the-rules-and-agents-system-making-the-process-repeatable.md"
      category: "Product Creation"

    posts:
      tuesday:
        publish_date: "2026-02-25"
        type: "article_announcement"
        content: |
          Here's the problem with most "systematic" product creation processes:

          They're not actually systematic. They're vibes-based decision-making disguised as process.

          "Does this feel like a good market?" → Vibes
          "Is this brand compelling?" → Vibes
          "Should we ship this feature?" → Vibes

          We built the opposite: 150+ explicit rules that define what "good" looks like at every stage.

          Examples:
          • Portfolio Gate: Ideas must score ≥20 across 8 criteria or they're killed (no discussion)
          • Durability Gate: Markets must score ≥18/25 or they fail (no exceptions)
          • Brand Gate: Visual identity must pass 12 specific criteria before build starts
          • Code Quality Gate: All code reviewed by security + bug prevention agents before merge

          These aren't guidelines. They're hard gates. If the rules aren't satisfied, work stops.

          The result? Zero ambiguity about what "done" means. Teams move faster because they're not debating standards—they're executing against them.

          And 25 specialized agents (human + AI) enforce the rules systematically.

          Full rules & agents breakdown in today's article (link in comments).

          #SystematicInnovation #ProcessDesign #ProductCreation #QualityGates

      thursday:
        publish_date: "2026-02-27"
        type: "engagement"
        content: |
          Unpopular opinion: Most "empowered teams" are actually just under-constrained teams.

          They have autonomy without clarity. Freedom without standards. They spend more time debating "what's good enough?" than shipping.

          Here's what actually empowers teams: Clear rules about what "done" looks like.

          When our teams know "this feature must pass 8 quality criteria before it ships," they don't debate quality standards—they execute. The rules create speed.

          The counterintuitive truth: Constraints don't slow teams down. Unclear constraints slow teams down.

          Give your team 150 explicit rules and they'll ship faster than teams with zero rules and "figure it out" guidance.

          What's your take? Do clear quality gates slow teams down or speed them up?

          #ProductLeadership #TeamEmpowerment #SystematicThinking

  # === WEEK 8 ===
  week_8:
    start_date: "2026-03-04"  # Tuesday
    article:
      title: "The Documentation System: Making Knowledge Repeatable and Auditable"
      slug: "the-documentation-system-repeatable-knowledge"
      file: "cultivate-source/the-documentation-system-making-knowledge-repeatable-and-auditable.md"
      category: "Product Creation"

    posts:
      tuesday:
        publish_date: "2026-03-04"
        type: "article_announcement"
        content: |
          I've watched Fortune 500 teams burn millions rebuilding knowledge that already existed in someone's head.

          The pattern is predictable:
          • Product Manager leaves → tribal knowledge disappears
          • New team joins → re-learns everything from scratch
          • Process that worked gets forgotten → team invents worse version
          • Decision rationale lost → same mistakes repeated 18 months later

          The root problem: Documentation is treated as an afterthought, not a systematic artifact.

          We built the opposite: a documentation system that captures every decision, every artifact, every quality gate outcome.

          12 structured document types:
          • Insight & Narrative Briefs (validation outcomes)
          • Durability Assessments (market scoring)
          • Brand System Documents (identity, messaging, visuals)
          • Moat Strategies (defensibility plans)
          • Feature Implementation Playbooks (how-to guides)
          • And 7 more...

          Every document is:
          ✓ Generated systematically by AI agents
          ✓ Validated against quality rules
          ✓ Version-controlled and auditable
          ✓ Integrated into the orchestration system

          The result: Knowledge compounds. Teams onboard faster. Processes are repeatable. And we never rebuild the same wheel.

          Full documentation system breakdown in today's article (link in comments).

          #KnowledgeManagement #SystematicWork #ProductCreation #ProcessDocumentation

      thursday:
        publish_date: "2026-03-06"
        type: "engagement"
        content: |
          Real talk: How much tribal knowledge would disappear if your top 3 people left tomorrow?

          Most teams vastly underestimate this risk. They think "our process is documented" but actually it's just a few Notion pages that no one updates.

          Here's the test:
          1. Pick a critical process (product validation, feature prioritization, whatever)
          2. Hand the docs to someone who's never done it
          3. Can they execute the process successfully without asking questions?

          If no, your documentation isn't actually documentation—it's organizational memory that will evaporate when people leave.

          We solve this by making docs first-class artifacts that AI agents generate systematically. Every quality gate produces structured documents. Every decision gets captured with full context.

          When someone leaves, their knowledge doesn't. It's already in the system.

          How does your team capture and preserve knowledge? What's your "bus factor"?

          #KnowledgeManagement #OrganizationalMemory #ProcessDesign

  # === WEEK 9 ===
  week_9:
    start_date: "2026-03-11"  # Tuesday
    article:
      title: "The Brand-First Validation: Designing Identity Before Code"
      slug: "the-brand-first-validation-identity-before-code"
      file: "cultivate-source/the-brand-first-validation-designing-identity-before-code.md"
      category: "Product Creation"

    posts:
      tuesday:
        publish_date: "2026-03-11"
        type: "article_announcement"
        content: |
          Most startups build the product first, then slap a brand on it later.

          This is backwards.

          Brand isn't decoration. It's a signal of product quality, positioning, and durability. And it's way easier to validate brand-market fit before you write a line of code.

          We built brand-first validation into our workflow:

          Gate 6 (Brand System): Before any product gets built, it must have:
          • A complete visual identity (logo, colors, typography)
          • A validated messaging system (positioning, value props, taglines)
          • A designed landing page
          • Validated aesthetic fit with the target market

          Why? Because brand signals durability.

          A generic "SaaS blue" brand signals low-trust, commodity positioning. A distinctive, well-crafted brand signals "this team sweats the details and builds quality products."

          We validate brand-market fit the same way we validate product-market fit: Put the brand in front of the target audience. Measure response. Iterate.

          By the time we write code, we already know the brand resonates. It's one less variable in the "will this succeed?" equation.

          Full brand-first validation framework in today's article (link in comments).

          #BrandStrategy #ProductCreation #DesignLeadership #StartupStrategy

      thursday:
        publish_date: "2026-03-13"
        type: "engagement"
        content: |
          Controversial take: A mediocre product with great branding beats a great product with mediocre branding.

          I've seen this pattern too many times:

          Team A: Builds incredible product, slaps generic SaaS brand on it, struggles to get traction. "Why isn't our superior product winning?"

          Team B: Builds decent product, invests in distinctive brand identity, messaging, positioning. Grows 3x faster.

          Why? Because brand is the first signal potential customers receive. It answers "is this worth my attention?" before they even try the product.

          A distinctive brand says: "This team is serious. This product is high-quality. This is worth evaluating."

          A generic brand says: "This is commodity software. Probably fine, but not exciting."

          This is why we validate brand-market fit before writing code. If the brand doesn't resonate, the product won't either—no matter how good it is.

          What's your take? How much does brand matter in SaaS? At what stage do you invest in it?

          #BrandStrategy #ProductStrategy #StartupMarketing

  # === WEEK 10 ===
  week_10:
    start_date: "2026-03-18"  # Tuesday
    article:
      title: "The Hub: A Meta-Project for Managing Your Product Portfolio"
      slug: "the-hub-managing-product-portfolio"
      file: "cultivate-source/the-hub-a-meta-project-for-managing-your-product-portfolio.md"
      category: "Product Creation"

    posts:
      tuesday:
        publish_date: "2026-03-18"
        type: "article_announcement"
        content: |
          Managing 16 product ideas across 8 quality gates with 25 AI agents is complex.

          So we built The Hub: a meta-project that manages the entire portfolio.

          Think of it as the "control tower" for product creation:

          • Single source of truth: Every product's status, gate outcomes, and next actions
          • Quality gate tracking: Which products passed which gates, which are blocked
          • Resource allocation: Where to deploy AI agents and human effort
          • Kill/Greenlight decisions: Real-time portfolio scoring and prioritization
          • Knowledge base: All briefs, strategies, playbooks, and docs centralized

          The Hub isn't just documentation—it's the operational system that orchestrates everything.

          When a product passes Gate 5 (Durability), The Hub automatically:
          1. Triggers Gate 6 (Brand System) agents
          2. Generates required brand documents
          3. Updates portfolio dashboard
          4. Routes next tasks to appropriate agents

          It's the difference between "managing products with Notion" and "systematic portfolio orchestration with zero manual overhead."

          And because it's built on the same workflow engine, The Hub itself was created using our product creation system. Meta, right?

          Full Hub breakdown in today's article (link in comments).

          #PortfolioManagement #ProductCreation #SystematicWork #Orchestration

      thursday:
        publish_date: "2026-03-20"
        type: "engagement"
        content: |
          Here's a question for product leaders managing multiple initiatives:

          How do you track which products are actually progressing vs. which are stuck?

          Most portfolio dashboards show:
          - Roadmap status ("On Track" / "At Risk" / "Delayed")
          - Revenue metrics (MRR, growth rate)
          - High-level milestones ("Beta shipped," "GA launched")

          But they don't show the actual quality gates that determine success:
          - Did this idea pass desirability validation?
          - Does the market have durability (18+ score)?
          - Is there a defensible moat?
          - Has brand-market fit been validated?

          We built The Hub to surface the gates that actually matter. At a glance, we can see:
          - 4 products in desirability validation (Gate 2-4)
          - 3 products passed durability (Gate 5), waiting for brand validation (Gate 6)
          - 2 products in build (Gate 7-8)

          This clarity changes everything. Resource allocation becomes obvious. Kill decisions become data-driven. No product languishes in "waiting for feedback" limbo.

          What metrics do you track to know if products are actually progressing?

          #ProductLeadership #PortfolioManagement #SystematicThinking

  # === WEEK 11 ===
  week_11:
    start_date: "2026-03-25"  # Tuesday
    article:
      title: "The Kill/Greenlight Ritual: Staying Ruthless About Portfolio Quality"
      slug: "the-kill-greenlight-ritual-portfolio-quality"
      file: "cultivate-source/the-kill-greenlight-ritual-staying-ruthless-about-portfolio-quality.md"
      category: "Product Creation"

    posts:
      tuesday:
        publish_date: "2026-03-25"
        type: "article_announcement"
        content: |
          We killed 8 product ideas in the last 6 months.

          Each one had passed initial validation. Each one had excited champions. Each one "felt" like a winner.

          But the data said otherwise. And we have a ritual that forces us to confront the data: The Kill/Greenlight Decision.

          Here's how it works:

          Every product must pass 8 quality gates. At each gate, we score it against explicit criteria. If it fails, it goes to Kill/Greenlight review.

          The review asks three questions:
          1. **What changed?** (Why is this failing now when it passed before?)
          2. **Can we fix it?** (Is there a pivot that salvages the core value?)
          3. **What's the opportunity cost?** (What are we NOT building if we keep this alive?)

          If the answers don't point to a clear path forward, we kill it. No exceptions. No "let's give it one more quarter."

          This ritual is what keeps our portfolio healthy. It prevents zombie products from consuming resources that could go to higher-value opportunities.

          The hardest part? Killing your own ideas. But the discipline is what makes the portfolio approach work.

          Full Kill/Greenlight ritual breakdown in today's article (link in comments).

          #ProductLeadership #PortfolioManagement #KillYourDarlings #StartupStrategy

      thursday:
        publish_date: "2026-03-27"
        type: "engagement"
        content: |
          Real question: How long do you give a product to prove itself before killing it?

          Most teams use vague criteria: "Let's see how it does in Q2." "Give it 6 more months." "Wait until the new feature ships."

          This creates zombie products—not dead, not thriving, just consuming resources.

          We use explicit gates with hard timelines:
          - Gate 2-4 (Desirability): 4 weeks max. If you can't validate demand in 4 weeks, the market isn't hot enough.
          - Gate 5 (Durability): 2 weeks. Score the market. If it's below 18/25, kill it.
          - Gate 6 (Brand): 3 weeks. Build + validate brand-market fit. If the brand doesn't resonate, positioning is wrong.
          - Gate 7-8 (Build): 8 weeks for MVP. If you can't ship an MVP in 8 weeks, scope is too big.

          No gate gets indefinite time. If you're stuck, you're either:
          1. Missing clarity about what "done" looks like (fix the gate criteria)
          2. Working on the wrong product (kill it)

          How does your team decide when to kill vs. keep iterating?

          #ProductLeadership #StartupStrategy #KillCriteria

  # === WEEK 12 ===
  week_12:
    start_date: "2026-04-01"  # Tuesday
    article:
      title: "The Results: What We've Learned Building This System"
      slug: "the-results-building-systematic-product-creation"
      file: "cultivate-source/the-results-what-we-ve-learned-building-this-system.md"
      category: "Product Creation"

    posts:
      tuesday:
        publish_date: "2026-04-01"
        type: "article_announcement"
        content: |
          Three years ago, I was failing at product creation. No process, no discipline, just "build and hope."

          Today, we're managing 16 products across 8 quality gates with 25 AI agents. We're killing bad ideas in weeks (not months). And we're shipping validated products systematically.

          Here's what we've learned:

          **1. Systems compound, chaos doesn't.**
          Every product we build makes the next one easier. The rules, agents, and documentation stack up. Chaos resets to zero every time.

          **2. Kill criteria are as important as build criteria.**
          Most teams know when to build. Few know when to stop. Our kill gates have saved more capital than our build process.

          **3. AI orchestration >> individual AI tools.**
          Having ChatGPT, Claude, and Cursor is great. Having an orchestrator that coordinates them systematically is transformative.

          **4. Documentation is a product, not a byproduct.**
          We generate 12 document types per product. Each one is validated, version-controlled, and integrated. Knowledge compounds.

          **5. Portfolio thinking reduces existential risk.**
          One product failing used to feel like the end. Now it's just data. Diversification is founder insurance.

          This system isn't perfect. But it's systematic, auditable, and improving every cycle.

          Full results + lessons learned in today's article (link in comments).

          #ProductCreation #SystematicInnovation #LessonsLearned #StartupStrategy

      thursday:
        publish_date: "2026-04-03"
        type: "engagement"
        content: |
          If you're building a product creation system from scratch, here's what I'd prioritize:

          **Year 1: Get the kill criteria right.**
          Most founders focus on "how do we build faster?" The real question is "how do we avoid building the wrong things?"

          Build your quality gates. Define your kill criteria. Get ruthless about saying no.

          **Year 2: Document everything.**
          Capture every decision, every artifact, every quality gate outcome. Make knowledge repeatable.

          Invest in structured documentation. Future you will thank past you.

          **Year 3: Orchestrate the AI tools.**
          Once you have quality gates + docs, add AI orchestration. Route tasks systematically. Enforce quality with agents.

          This is where velocity compounds. But you need the foundation first.

          The mistake? Starting with AI. Tools are multipliers, not foundations. If your process is chaotic, AI just makes chaos faster.

          What would you prioritize if you were building a systematic product creation workflow?

          #ProductCreation #SystematicThinking #ProcessDesign #AIOrchestration

# === TRANSITION TO ADDITIONAL CONTENT ===
# After Week 12, continue with remaining Cultivate articles or evergreen content
# Potential articles to add:
# - The Technical Stack
# - The Supabase MCP
# - The Dev Quality Assistant
# - The Security and Bug Gate
# - The Gamification That Doesn't Manipulate
# - The Micro-Interactions
# - The Personalization with Privacy
# - Tool-specific deep dives (Glif, Lindy, Context7)
